{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 3\n",
    "Автор материала: Павел Нестеров (@mephistopheies). Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/100c3Ek94UL-VRwXrN4lxCSnGjfJrl6Gc96G21DNCh4w).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн-версию алгоритма multilabel-классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн-моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно.\n",
    "\n",
    "PS2:\n",
    "- в процессе решения домашней работы вам придется работать с текстом, и у вас может возникнуть желание сделать очевидный препроцессинг, например привести все слова в нижний регистр, в-общем **этого делать не нужно, если не оговорено заранее в задании**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем версии используемых библиотек. Совпадут ли ответы в случае других версий - не гарантируется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.3\n",
      "IPython 6.1.0\n",
      "\n",
      "numpy 1.13.3\n",
      "scipy 0.19.1\n",
      "pandas 0.20.3\n",
      "matplotlib 2.1.0\n",
      "sklearn 0.19.1\n",
      "\n",
      "compiler   : GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 17.3.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   : e24ab1eb00a8599cfb9ecdb21dac957d4d3c4db8\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watermark output of author:\n",
    "CPython 3.6.1\n",
    "IPython 5.3.0\n",
    "\n",
    "numpy 1.14.0\n",
    "scipy 1.0.0\n",
    "pandas 0.22.0\n",
    "matplotlib 2.1.2\n",
    "sklearn 0.19.1\n",
    "\n",
    "compiler   : GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)\n",
    "system     : Darwin\n",
    "release    : 17.4.0\n",
    "machine    : x86_64\n",
    "processor  : i386\n",
    "CPU cores  : 8\n",
    "interpreter: 64bit\n",
    "Git hash   : 45c8c34462bbf54d2a05de99fb9f0a6491fa1e27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = '../../data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = '../../data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'php', 'ios', 'java', 'javascript', 'c#', 'python', 'c++', 'html', 'android', 'jquery'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания; если предпочитаете текст, то и он есть [тут](https://www.ics.uci.edu/~pjsadows/notes.pdf) и [тут](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "<font color=\"red\">Вопрос 1.</font> Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Ответ: 2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "<font color=\"red\">Вопрос 2.</font>В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$. Какой вид она будет иметь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Ответ: 2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$, если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. В нашем случае, чтобы не пересчитывать [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) самим или с помощью [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), мы будем идти по словам предложения в порядке их следования. Если какое-то слово встречается несколько раз, то мы добавляем его в аккумулятор со своим весом. В итоге получится то же самое, как если сначала посчитать количество одинаковых слов и домножить на соответствующий вес. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку и реализовать $\\sigma$ без риска overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "\n",
    "                    sigma = 1/(1+np.exp(-z)) if z>=0 else 1-1/(1+np.exp(z))\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma_safe = sigma\n",
    "                    if sigma < tolerance:\n",
    "                        sigma_safe = tolerance\n",
    "                    if 1-sigma < tolerance:\n",
    "                        sigma_safe = 1-tolerance\n",
    "                    sample_loss += -(y*np.log(sigma_safe) + (1-y)*(np.log(1-sigma_safe)))\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = y-sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "              #  if n == total:\n",
    "              #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1e0381bd3f432bb6632085c7236c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 тысяч примеров, чтобы хоть как-то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5kAAAKnCAYAAAAMUYlPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1b583c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 19.65\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 3.</font>\n",
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Ответ: 3</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        jakkard_indexes = []\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                # предсказанные тэги\n",
    "                predicted_tags = set([])\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "\n",
    "                    sigma = 1/(1+np.exp(-z)) if z>=0 else 1-1/(1+np.exp(z))\n",
    "                    \n",
    "                    # если мы в режиме тестирования добавим тэг в предсказанные если вероятность этого >0.9\n",
    "                    if n >= top_n_train:\n",
    "                        if sigma > 0.9:\n",
    "                            predicted_tags.add(tag)\n",
    "                            \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma_safe = sigma\n",
    "                    if sigma < tolerance:\n",
    "                        sigma_safe = tolerance\n",
    "                    if 1-sigma < tolerance:\n",
    "                        sigma_safe = 1-tolerance\n",
    "                    sample_loss += -(y*np.log(sigma_safe) + (1-y)*(np.log(1-sigma_safe)))\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = y-sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "                if n >= top_n_train:\n",
    "                    jakkard_indexes.append(len(tags & predicted_tags) / len(tags | predicted_tags))\n",
    "\n",
    "        return np.mean(jakkard_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48ffa7acfd64be388fa91d457aa5462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.59\n"
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 4.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=\"red\">Ответ: 3</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической корректности мы используем грязный трюк: будем регуляризировать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение (bias) не регуляризируется. `sample_loss` тоже должен остаться без изменений.\n",
    "\n",
    "Замечание:\n",
    "- не забудьте, что нужно учитывать регуляризацию слова в градиентном шаге только один раз\n",
    "- условимся, что учитываем регуляризацию только при первой встрече слова\n",
    "- если бы мы считали сначала bag-of-words, то мы бы в цикле шли по уникальным словам, но т.к. мы этого не делаем, приходится выкручиваться (еще одна жертва богу online-моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 5.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Регуляризация ElasticNet , реализация\n",
    "\n",
    "В качестве седьмой задачи вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной `ElasticNet`-регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 7.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность появления каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 8.</font> Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что не удивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. c# \n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре – 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов, используя данные из ``train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 9.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение этого задания вам предлагается реализовать метод `predict_proba`, который принимает строку, содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(model.predict_proba(sentence).items(), \n",
    "       key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Отметьте все теги, ассоциирующиеся с данным вопросом, если порог принятия равен $0.9$. То есть считаем, что вопросу надо поставить некоторый тег, если вероятность его появления, предсказанная моделью, больше или равна 0.9. \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. php\n",
    "4. java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
