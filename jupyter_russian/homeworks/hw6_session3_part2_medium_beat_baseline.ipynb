{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Open Machine Learning Course\n",
    "<center>\n",
    "Author: Yury Kashnitsky, Data Scientist at Mail.Ru Group\n",
    "\n",
    "This material is subject to the terms and conditions of the license [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Free use is permitted for any non-comercial purpose with an obligatory indication of the names of the authors and of the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Assignment #6\n",
    "### <center> Beating benchmarks in \"How good is your Medium article?\"\n",
    "    \n",
    "[Competition](https://www.kaggle.com/c/how-good-is-your-medium-article). The task is to beat \"Assignment 6 baseline\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer\n",
    "import scipy.sparse \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help to throw away all HTML tags from an article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary function to read a JSON line without crashing on escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features `content`, `published`, `title` and `author`, write them to separate files for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_and_write(path_to_data,\n",
    "                               inp_filename, is_train=True):\n",
    "    \n",
    "    features = ['content', 'published', 'title', 'author']\n",
    "    prefix = 'train' if is_train else 'test'\n",
    "    feature_files = [open(os.path.join(path_to_data,\n",
    "                                       '{}_{}.txt'.format(prefix, feat)),\n",
    "                          'w', encoding='utf-8')\n",
    "                     for feat in features]\n",
    "   \n",
    "    totals = {'test': 34645, 'train': 62313}\n",
    "    with open(os.path.join(path_to_data, inp_filename), \n",
    "              encoding='utf-8') as inp_json_file:\n",
    "\n",
    "        for line in tqdm_notebook(inp_json_file, total=totals[prefix]):\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            title = json_data['title'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            print(content_no_html_tags, file=feature_files[0])\n",
    "            print(json_data['published']['$date'], file=feature_files[1])\n",
    "            print(title, file=feature_files[2])\n",
    "            print(json_data['meta_tags']['author'], file=feature_files[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '/Users/lucky/.kaggle/competitions/how-good-is-your-medium-article' # modify this if you need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_file(prefix, feature):\n",
    "    return os.path.join(PATH_TO_DATA, '{}_{}.txt'.format(prefix, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(feature_file('train', 'content')):\n",
    "    extract_features_and_write(PATH_TO_DATA, 'train.json', is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(feature_file('test', 'content')):\n",
    "    extract_features_and_write(PATH_TO_DATA, 'test.json', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the following groups of features:**\n",
    "    - Tf-Idf with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Tf-Idf with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Time features: publication hour, whether it's morning, day, night, whether it's a weekend\n",
    "    - Bag of authors (i.e. One-Hot-Encoded author names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 10.3 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Hashing of train dataset has been completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Hashing of test dataset has been completed\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "X_train_content_sparse, X_train_title_sparse, X_train_author_sparse, X_train_time_features_sparse = (None, None, None, None)\n",
    "X_test_content_sparse, X_test_title_sparse, X_test_author_sparse, X_test_time_features_sparse = (None, None, None, None)\n",
    "    \n",
    "hv = HashingVectorizer(non_negative=True, norm=None, n_features=100000, ngram_range=(1,3))\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "with open(feature_file('train', 'content'), encoding='utf-8') as input_train_file:\n",
    "    X_train_content_sparse = hv.transform(input_train_file)\n",
    "    print('[-] Hashing of train dataset has been completed')\n",
    "#    X_train_content_sparse = tfidf.fit_transform(train_vectors)\n",
    "#    print('[+] TFxIDF transform is done')\n",
    "\n",
    "with open(feature_file('test', 'content'), encoding='utf-8') as input_test_file:\n",
    "    X_test_content_sparse = hv.transform(input_test_file)\n",
    "    print('[-] Hashing of test dataset has been completed')\n",
    "#    X_test_content_sparse = tfidf.transform(test_vectors)\n",
    "#    print('[+] TFxIDF transform is done')\n",
    "    \n",
    "\n",
    "tv = TfidfVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "with open(feature_file('train', 'title'), encoding='utf-8') as input_train_file:\n",
    "    X_train_title_sparse = tv.fit_transform(input_train_file)\n",
    "\n",
    "with open(feature_file('test', 'title'), encoding='utf-8') as input_test_file:\n",
    "    X_test_title_sparse = tv.transform(input_test_file)\n",
    "\n",
    "\n",
    "tv1 = TfidfVectorizer(binary=True, use_idf=False, norm=None, token_pattern='[^\\n]+')\n",
    "with open(feature_file('train', 'author'), 'rt') as input_train_file:\n",
    "    X_train_author_sparse = tv1.fit_transform(input_train_file)\n",
    "    \n",
    "with open(feature_file('test', 'author'), 'rt') as input_test_file:\n",
    "    X_test_author_sparse = tv1.transform(input_test_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_features(filename):\n",
    "    x = pd.read_csv(filename, header=None)\n",
    "    x.columns = ['published']\n",
    "    tf = pd.DataFrame()\n",
    "    x.published = x.published.astype('datetime64[ns]')\n",
    "    tf['hour_of_day'] = x.published.dt.hour\n",
    "    tf['day_of_week'] = x.published.dt.dayofweek\n",
    "    tf['day_of_year'] = x.published.dt.dayofyear\n",
    "    tf['is_friday_saturday'] = tf.day_of_week.apply(lambda d: 1 if d==4 or d==5 else 0)\n",
    "    tf['part_of_day'] = tf['hour_of_day'].apply(lambda h: 1 if h > 5 and h <= 11 else (\n",
    "                                                     2 if h > 11 and h <= 17 else (\n",
    "                                                     3 if h > 17 and h <= 23 \n",
    "                                                         else 4)))\n",
    "    tf['top_hours'] = tf['hour_of_day'].apply(lambda h: 1 if h in (1, 5, 9, 14) else 0)\n",
    "    return tf\n",
    "\n",
    "X_train_time_features = get_time_features(feature_file('train', 'published'))\n",
    "X_test_time_features = get_time_features(feature_file('test', 'published'))\n",
    "\n",
    "encoder = OneHotEncoder(categorical_features='all')\n",
    "X_train_time_features_sparse = encoder.fit_transform(X_train_time_features)\n",
    "X_test_time_features_sparse = encoder.transform(X_test_time_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join all sparse matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 100000), (62313, 100000), (62313, 31319), (62313, 405))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_content_sparse.shape, X_train_title_sparse.shape, X_train_author_sparse.shape, X_train_time_features_sparse.shape\n",
    "#X_train_sparse = csr_matrix(hstack([X_train_content_sparse, X_train_title_sparse,\n",
    "                                    #X_train_author_sparse]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sparse = csr_matrix(hstack([X_train_content_sparse, X_train_title_sparse,\n",
    "                                    X_train_author_sparse, X_train_time_features_sparse]))\n",
    "X_test_sparse = csr_matrix(hstack([X_test_content_sparse, X_test_title_sparse,\n",
    "                                    X_test_author_sparse, X_test_time_features_sparse]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read train target and split data for validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), \n",
    "                           index_col='id')\n",
    "y_train = train_target['log_recommends'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find out correlation between some time features and the target attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(hour_of_day\n",
       " 0.0     194.080199\n",
       " 1.0     461.213947\n",
       " 2.0     305.840884\n",
       " 3.0     339.406281\n",
       " 4.0     155.343964\n",
       " 5.0     583.033277\n",
       " 6.0     255.599988\n",
       " 7.0     295.258059\n",
       " 8.0     369.778356\n",
       " 9.0     566.212346\n",
       " 10.0    252.296197\n",
       " 11.0    279.795048\n",
       " 12.0    267.701192\n",
       " 13.0    313.730345\n",
       " 14.0    427.061511\n",
       " 15.0    363.566415\n",
       " 16.0    276.849522\n",
       " 17.0    335.731069\n",
       " 18.0    359.708586\n",
       " 19.0    241.422629\n",
       " 20.0    363.857726\n",
       " 21.0    316.952048\n",
       " 22.0    318.374608\n",
       " 23.0    321.751181\n",
       " Name: log_recommends, dtype: float64, day_of_week\n",
       " 0.0    328.316343\n",
       " 1.0    322.094256\n",
       " 2.0    338.987363\n",
       " 3.0    291.908646\n",
       " 4.0    350.506525\n",
       " 5.0    386.893680\n",
       " 6.0    307.986354\n",
       " Name: log_recommends, dtype: float64, is_friday_saturday\n",
       " 0.0    319.370722\n",
       " 1.0    363.392023\n",
       " Name: log_recommends, dtype: float64, top_hours\n",
       " 0.0    303.209582\n",
       " 1.0    486.138853\n",
       " Name: log_recommends, dtype: float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([X_train_time_features, train_target['log_recommends'].apply(np.expm1)], axis=1)\n",
    "df.groupby('hour_of_day').log_recommends.mean(), df.groupby('day_of_week').log_recommends.mean(), \\\n",
    "df.groupby('is_friday_saturday').log_recommends.mean(), df.groupby('top_hours').log_recommends.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * train_target.shape[0])\n",
    "X_train_part_sparse = X_train_sparse[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "X_valid_sparse =  X_train_sparse[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a simple Ridge model and check MAE on the validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2472784149842058, 2.4808566071786915)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(random_state=17)\n",
    "ridge.fit(X_train_part_sparse, y_train_part);\n",
    "ridge_pred = ridge.predict(X_valid_sparse)\n",
    "valid_mae = mean_absolute_error(y_valid, ridge_pred)\n",
    "valid_mae, np.expm1(valid_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with time features: (1.3055035609096739, 2.6895465374084075) - 1.62456 was 1.62666\n",
    "* with is_friday_saturday,day_of_year,top_hours: (1.305855113415952, 2.6908438347613699) - 1.62321 was 1.62456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40min 44s, sys: 16.6 s, total: 41min\n",
      "Wall time: 41min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge.fit(X_train_sparse, y_train);\n",
    "ridge_test_pred = ridge.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename,\n",
    "                          path_to_sample='sample_submission.csv'):\n",
    "    submission = pd.read_csv(os.path.join(PATH_TO_DATA, path_to_sample), index_col='id')\n",
    "    \n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_submission_file(ridge_test_pred, 'assignment6_medium_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now's the time for dirty Kaggle hacks. Form a submission file with all zeroes. Make a submission. What do you get if you think about it? How is it going to help you with modifying your predictions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_submission_file(np.zeros_like(ridge_test_pred), \n",
    "                      'medium_all_zeros_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify predictions in an appropriate way (based on your all-zero submission) and make a new submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.125392289172281, 4.3332799999999994)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_test_mean = 4.33328\n",
    "ridge_test_pred_modif = ridge_test_pred + (kaggle_test_mean - ridge_test_pred.mean())\n",
    "ridge_test_pred.mean(), ridge_test_pred_modif.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_test_pred_modif = ridge_test_pred + (kaggle_test_mean - ridge_test_pred.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_submission_file(ridge_test_pred_modif, \n",
    "                      'assignment6_medium_submission_with_hack.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "1e3647b92d1d422a98507cc55542871f": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
