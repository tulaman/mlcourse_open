{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Open Machine Learning Course\n",
    "<center>\n",
    "Author: Yury Kashnitsky, Data Scientist at Mail.Ru Group\n",
    "\n",
    "This material is subject to the terms and conditions of the license [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Free use is permitted for any non-comercial purpose with an obligatory indication of the names of the authors and of the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Assignment #6. Part 1\n",
    "### <center> Beating benchmarks in \"Catch Me If You Can: Intruder Detection through Webpage Session Tracking\"\n",
    "    \n",
    "[Competition](https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2). The task is to beat \"Assignment 6 baseline\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = ('/Users/lucky/.kaggle/competitions/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2')\n",
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare our dataset. Replace N/A with 0 values and convert columns to appropriate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id\n",
       "1   -1792 days +17:33:01.666666\n",
       "2               0 days 00:00:03\n",
       "3        0 days 00:00:00.888888\n",
       "4        0 days 00:01:03.333333\n",
       "5        0 days 00:00:43.555555\n",
       "dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_train_df = train_df.iloc[:5]\n",
    "toy_test_df = test_df.iloc[:5]\n",
    "\n",
    "\n",
    "toy_train_sites = toy_train_df[['site'+str(i) for i in range(1, 11)]]\n",
    "toy_test_sites = toy_test_df[['site'+str(i) for i in range(1, 11)]]\n",
    "toy_full_sites = toy_test_sites.append(toy_train_sites)\n",
    "\n",
    "toy_train_sessions = toy_train_sites.fillna(0).astype(int).astype(str).apply(lambda s: ' '.join(s), axis=1)\n",
    "toy_test_sessions = toy_test_sites.fillna(0).astype(int).astype(str).apply(lambda s: ' '.join(s), axis=1)\n",
    "toy_full_sessions = toy_full_sites.fillna(0).astype(int).astype(str).apply(lambda s: ' '.join(s), axis=1)\n",
    "#print(toy_full_sessions)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=100000)\n",
    "vectorizer = vectorizer.fit(toy_full_sessions)\n",
    "toy_train_v = vectorizer.transform(toy_train_sessions)\n",
    "toy_test_v = vectorizer.transform(toy_test_sessions)\n",
    "\n",
    "vstack([toy_test_v, toy_train_v])\n",
    "\n",
    "toy_train_times = toy_train_df[['time'+str(i) for i in range(1, 11)]]\n",
    "toy_train_times.loc[:, ['time2','time1']]\n",
    "toy_train_times['delay9'] = toy_train_times['time10']-toy_train_times['time9']\n",
    "toy_train_times['delay8'] = toy_train_times['time9']-toy_train_times['time8']\n",
    "toy_train_times['delay7'] = toy_train_times['time8']-toy_train_times['time7']\n",
    "toy_train_times['delay6'] = toy_train_times['time7']-toy_train_times['time6']\n",
    "toy_train_times['delay5'] = toy_train_times['time6']-toy_train_times['time5']\n",
    "toy_train_times['delay4'] = toy_train_times['time5']-toy_train_times['time4']\n",
    "toy_train_times['delay3'] = toy_train_times['time4']-toy_train_times['time3']\n",
    "toy_train_times['delay2'] = toy_train_times['time3']-toy_train_times['time2']\n",
    "toy_train_times['delay1'] = toy_train_times['time2']-toy_train_times['time1']\n",
    "toy_train_times['delay1'] = 0 \n",
    "np.mean(toy_train_times[['delay1','delay2','delay3','delay4','delay5', 'delay6', 'delay7', 'delay8', 'delay9']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    df = df.fillna(0)\n",
    "    for i in range(1, 11):\n",
    "        df['site' + str(i)] = df['site' + str(i)].astype(int)\n",
    "        df['time' + str(i)] = pd.to_datetime(df['time' + str(i)])\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = prepare_dataset(train_df)\n",
    "test_df = prepare_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate target feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.6 s, sys: 941 ms, total: 50.6 s\n",
      "Wall time: 51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = TfidfVectorizer(ngram_range=(1, 3), max_features=100000)\n",
    "\n",
    "train_sites = X_train[['site'+str(i) for i in range(1, 11)]]\n",
    "val_sites = X_val[['site'+str(i) for i in range(1, 11)]]\n",
    "test_sites = test_df[['site'+str(i) for i in range(1, 11)]]\n",
    "#full_sites = test_sites.append(train_sites)\n",
    "\n",
    "train_sessions = train_sites.astype(str).apply(lambda s: ' '.join(s), axis=1)\n",
    "val_sessions = train_sites.astype(str).apply(lambda s: ' '.join(s), axis=1)\n",
    "test_sessions = test_sites.astype(str).apply(lambda s: ' '.join(s), axis=1)\n",
    "full_sessions = test_sessions.append(train_sessions.append(val_sessions))\n",
    "    \n",
    "vec = vec.fit(full_sessions)\n",
    "\n",
    "train_v = vec.transform(train_sessions)\n",
    "val_v = vec.transform(val_sessions)\n",
    "test_v = vec.transform(test_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features based on the session start time: hour, whether it's morning, day or night and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def analyze_session_start(df):\n",
    "    df['session_hour'] = df['time1'].dt.hour\n",
    "    df['session_pod'] = df['session_hour'].apply(lambda h: 1 if h > 6 and h <= 12 else (\n",
    "                                                     2 if h > 12 and h <= 18 else (\n",
    "                                                     3 if h > 18 and h <= 24 \n",
    "                                                         else 4)))\n",
    "    df['session_dow'] = df['time1'].dt.dayofweek\n",
    "    df['is_weekend'] = df['session_dow'].apply(lambda d: 1 if d==5 or d==6 else 0)\n",
    "    return df[['session_hour', 'session_pod', 'session_dow', 'is_weekend']]\n",
    "    \n",
    "time_features_train = analyze_session_start(X_train)\n",
    "time_features_val = analyze_session_start(X_val)\n",
    "time_features_test = analyze_session_start(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale this features and combine then with Tf-Idf based on sites (you'll need `scipy.sparse.hstack`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "time_features_train = encoder.fit_transform(time_features_train)\n",
    "time_features_val = encoder.fit_transform(time_features_val)\n",
    "time_features_test = encoder.transform(time_features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "scaler = scaler.fit(time_features_train)\n",
    "time_features_train = scaler.transform(time_features_train)\n",
    "time_features_val = scaler.transform(time_features_val)\n",
    "time_features_test = scaler.transform(time_features_test)    \n",
    "\n",
    "train_dataset = hstack([time_features_train, train_v])\n",
    "val_dataset = hstack([time_features_val, val_v])\n",
    "test_dataset = hstack([time_features_test, test_v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform cross-validation with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 100000 features per sample; expecting 100027",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0mcalculate_ovr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ovr\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcalculate_ovr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 305\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 100000 features per sample; expecting 100027"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "logitCV = LogisticRegressionCV(\n",
    "        Cs=10,\n",
    "        penalty='l1',\n",
    "        scoring='roc_auc',\n",
    "        cv=skf,\n",
    "        random_state=42,\n",
    "        solver='liblinear',\n",
    "        n_jobs=-1\n",
    "#        tol=10\n",
    "    )\n",
    "\n",
    "logitCV.fit(train_dataset, y_train)\n",
    "\n",
    "\n",
    "os.system('say \"your program has finished\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max auc_roc: 0.998155600268\n"
     ]
    }
   ],
   "source": [
    "print ('AUC_ROC for our model:', roc_auc_score(y_val, logitCV.predict_proba(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction for the test set and form a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = logitCV.predict_proba(test_dataset)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(test_pred, \"assignment6_alice_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
